{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5202 Assessment 2-B\n",
    "\n",
    "### Producer File for Task-2\n",
    "\n",
    "#### Student Name: Akash Goyal\n",
    "#### Student ID: 30749964"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Libraries used:** the main libraries/modules used in my assignment are:\n",
    "* `os` Setting the configuration for the os\n",
    "* `SparkConf` So that the Spark configurations can be set\n",
    "* `SparkContext` To be able to create a spark context object\n",
    "* `SparkSession` To be able to build a spark session object and set the timezone\n",
    "* `pyspark.sql.function` package to be able to use spark functions to manipulate data\n",
    "* `sleep` So that we can use the sleep function to introduce delay\n",
    "* `pyspark.sql.types` Import different types of DataTypes\n",
    "* `PipelineModel` To use Pipeline to train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Importing Libraries](#lib)\n",
    "* [Introduction](#intro)\n",
    "* [1. Creation of SparkSession](#sp)\n",
    "* [2. Sensor Data Ingestion](#2.sdinges)\n",
    "* [3. Reading the Pedestrian Data](#rdped)\n",
    "* [4. Persist the streaming data in parquet](#4)\n",
    "* [5. Transforming data into format](#5)\n",
    "* [6. Transformations to prepare the columns for model prediction](#6)\n",
    "* [7. Modelling Building](#7)\n",
    "* [8. Aggregating data](#8)\n",
    "    * [a. Aggregating on days](#8a) \n",
    "    * [b. Publishing the data to kafka topic](#8b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries <a class=\"anchor\" name=\"lib\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "# So that the Spark configurations can be set\n",
    "from pyspark import SparkConf\n",
    "# To be able to create a spark context object\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "# Import functions package\n",
    "from pyspark.sql import functions as F\n",
    "# Import datetime for using datetime functions\n",
    "from datetime import datetime, timedelta\n",
    "# Import different types of DataTypes \n",
    "from pyspark.sql.types import *\n",
    "from time import sleep\n",
    "\n",
    "#To use Pipeline to train model\n",
    "from pyspark.ml.pipeline import PipelineModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a class=\"anchor\" name=\"intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we have to use Spark to read the stream of data published on the kafka topic **'pedestrianData'** and manipulate the data to get it in shape to be ready for it to get transformed using the given model. The model would then be used to predict the pedestrian counts for the sensors. From there the data would again be written to the kafka topic **'final_joined_df'** for it to be consumed for visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creation of SparkSession <a class=\"anchor\" name=\"sp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we were required to create the spark context from the Spark Session. So in the below code it can be seen that the first the configurations for the spark sessionobject has been created. The configurations that have been set here are:\n",
    "* The number of cores/processors that we want to use. This is set in a **master** variable. In the below case **local[2]** has been used which means that two processors that are there in the machine need to be used.\n",
    "* Next we need to set the appropriate application name.\n",
    "\n",
    "Using the above configuration values we can create a spark session object. But it is also required to set the UTC time-zone. To set this we need to make changes to the property **spark.sql.session.timeZone**. We need to set 'UTC' to the property as shown in the code below.\n",
    "\n",
    "After this finally the spark context object can be created and then the log level can be set to **ERROR** so that not a lot of log generation takes place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For selecting the number of cores that we want spark to use\n",
    "# in this case we are asking it to use 2 cores\n",
    "master = \"local[2]\"\n",
    "\n",
    "# Putting the Application name\n",
    "app_name = \"Streaming application using Spark Structured Streaming\"\n",
    "\n",
    "# Setting up the configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "# Using SparkSession\n",
    "# Setting the time-zone\n",
    "spark = SparkSession.builder.config(conf=spark_conf).config('spark.sql.session.timeZone', 'UTC').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sensor Data Ingestion <a class=\"anchor\" name=\"sdinges\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the assignment it is required to load the given data in a CSV files into Spark DataFrames with appropriate schema as given in the metadata file.\n",
    "\n",
    "For this first we need to create the schema according to the metadata file with the exception of **location** column in the sensor data file. The schema can be created using the **StructType()** and the **StructField()** functions.\n",
    "the complete schema for each dataframe would be created inside the StructType() function and to add a new field and its datatype the StructField datatype is used.\n",
    "\n",
    "Below is the schema for the sensor data with the appropriate datatypes as mentioned in the metadata file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema for the sensor data\n",
    "schema_sensor_data_df = StructType([\n",
    "    StructField(\"sensor_id\", IntegerType()),\n",
    "    StructField(\"sensor_description\", StringType()),\n",
    "    StructField(\"sensor_name\", StringType()),\n",
    "    StructField(\"installation_date\", DateType()),\n",
    "    StructField(\"status\", StringType()),\n",
    "    StructField(\"note\", StringType()),\n",
    "    StructField(\"direction_1\", StringType()),\n",
    "    StructField(\"direction_2\", StringType()),\n",
    "    StructField(\"latitude\", FloatType()),\n",
    "    StructField(\"longitude\", FloatType()),\n",
    "    StructField(\"location\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Pedestrian_Counting_System_-_Sensor_Locations.csv to the dataframe: sensor_spdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor_id: integer (nullable = true)\n",
      " |-- sensor_description: string (nullable = true)\n",
      " |-- sensor_name: string (nullable = true)\n",
      " |-- installation_date: date (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- note: string (nullable = true)\n",
      " |-- direction_1: string (nullable = true)\n",
      " |-- direction_2: string (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading the sensor data\n",
    "sensor_spdf = spark.read.format('csv').schema(schema_sensor_data_df)\\\n",
    "                         .option('header',True).option('escape','\"').option(\"dateFormat\",'yyyy/MM/dd')\\\n",
    "                         .load('Pedestrian_Counting_System_-_Sensor_Locations.csv')\n",
    "# Print the schema of the sensor dataframe\n",
    "sensor_spdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reading the Pedestrian Data <a class=\"anchor\" name=\"rdped\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the task we would be reading the streaming data from the **'pedestrianData'** kafka topic using:\n",
    "* sparks readstream API\n",
    "* To read the data from kafka we need to use the the format() function with **'kafka'** parameter.\n",
    "* option(\"kafka.bootstrap.servers\", \"localhost:9092\") - To set the server to be used.\n",
    "* option(\"subscribe\", topic) - Option to set the kafka topic from the where the data needs to be read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic = 'pedestrianData'\n",
    "\n",
    "# Reading the data from the kafka topic\n",
    "pdstrn_str_df = spark.readStream \\\n",
    "                 .format(\"kafka\") \\\n",
    "                 .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "                 .option(\"subscribe\", topic) \\\n",
    "                 .load()\n",
    "\n",
    "pdstrn_str_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Persist the raw streaming data in parquet format <a class=\"anchor\" name=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the assignment task its required for us to persist the raw data into **parquet** format as it is a file system that stores tha data in tbular format and can efficiently read back. It helps in the cases of big data. Its much efficient as compared to the row based csv files.\n",
    "Below are the things used:\n",
    "* Removal of the checkpoint: Though theoretically there is no need to remove the checkpoint files, but it is causing problems when writing the data to the sink. It was giving errors on the sparkUI. Therefore the code to remove the checkpoint file has been added.\n",
    "    * rm: To remove the file\n",
    "    * -rf : -r to remove the file and -f to force it in a quiet manner ie. without display.\n",
    "* sparks writestream API\n",
    "* To write the data in parquet format we need to use the the format() function with **'parquet'** parameter.\n",
    "* outputMode - 'append' : In this mode only the new rows will added to the result table.\n",
    "* option(path): The file path where the result table would get stored.\n",
    "* option(checkpointLocation): With checkpointing the state it would help to recover the last state.\n",
    "* start() : To start writing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf pdstrn_str_df_parquet/pdstrn_df_chkpnt\n",
    "\n",
    "# Writing to the parquet file\n",
    "pdstrn_parquet_sink = pdstrn_str_df.writeStream\\\n",
    "                           .format(\"parquet\")\\\n",
    "                           .outputMode(\"append\")\\\n",
    "                           .option(\"path\", \"pdstrn_str_df_parquet\")\\\n",
    "                           .option(\"checkpointLocation\", \"pdstrn_str_df_parquet/pdstrn_df_chkpnt\")\\\n",
    "                           .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the file sink query after some time\n",
    "# pdstrn_parquet_sink.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transforming data into format <a class=\"anchor\" name=\"5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the task we need to create a process which would make sure that the data would get transformed into the structure/schema as the metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can convert the value got from the kafka topic into string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the data into string\n",
    "pdstrn_stream_df = pdstrn_str_df.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for pedestrian count data\n",
    "# Taking the datatypes for all the attributes\n",
    "# as string as the data is given as array of jsons\n",
    "pedestrian_schema =  ArrayType(StructType([  \n",
    "    StructField(\"ID\", StringType(), True),\n",
    "    StructField(\"Date_Time\", StringType(), True),\n",
    "    StructField(\"Year\", StringType(), True),\n",
    "    StructField(\"Month\", StringType(), True),\n",
    "    StructField(\"Mdate\", StringType(), True),\n",
    "    StructField(\"Day\", StringType(), True),\n",
    "    StructField(\"Time\", StringType(), True),\n",
    "    StructField(\"Sensor_ID\", StringType(), True),\n",
    "    StructField(\"Sensor_Name\", StringType(), True),\n",
    "    StructField(\"Hourly_Counts\", StringType(), True)        \n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- ID: string (nullable = true)\n",
      " |    |    |-- Date_Time: string (nullable = true)\n",
      " |    |    |-- Year: string (nullable = true)\n",
      " |    |    |-- Month: string (nullable = true)\n",
      " |    |    |-- Mdate: string (nullable = true)\n",
      " |    |    |-- Day: string (nullable = true)\n",
      " |    |    |-- Time: string (nullable = true)\n",
      " |    |    |-- Sensor_ID: string (nullable = true)\n",
      " |    |    |-- Sensor_Name: string (nullable = true)\n",
      " |    |    |-- Hourly_Counts: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converting the json data into the schema created above and naming the column data \n",
    "pdstrn_df = pdstrn_stream_df.select(F.from_json(F.col(\"value\"),\n",
    "                                                pedestrian_schema)\\\n",
    "                                                .alias('data'))\n",
    "\n",
    "pdstrn_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to explode the data out of the json array structure so that the values can be parsed to their actual datatype instead of string data. This can be done using the **explode()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- elements: struct (nullable = true)\n",
      " |    |-- ID: string (nullable = true)\n",
      " |    |-- Date_Time: string (nullable = true)\n",
      " |    |-- Year: string (nullable = true)\n",
      " |    |-- Month: string (nullable = true)\n",
      " |    |-- Mdate: string (nullable = true)\n",
      " |    |-- Day: string (nullable = true)\n",
      " |    |-- Time: string (nullable = true)\n",
      " |    |-- Sensor_ID: string (nullable = true)\n",
      " |    |-- Sensor_Name: string (nullable = true)\n",
      " |    |-- Hourly_Counts: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exploding the data column into elements\n",
    "pdstrn_df = pdstrn_df.select(F.explode(F.col(\"data\"))\\\n",
    "                     .alias('elements'))\n",
    "\n",
    "pdstrn_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to rename the columns using the **alias()** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Date_Time: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Mdate: string (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Sensor_ID: string (nullable = true)\n",
      " |-- Sensor_Name: string (nullable = true)\n",
      " |-- Hourly_Counts: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdstrn_df = pdstrn_df.select(pdstrn_df.elements.ID.alias(\"ID\"),\n",
    "                             pdstrn_df.elements.Date_Time.alias(\"Date_Time\"),\n",
    "                             pdstrn_df.elements.Year.alias(\"Year\"),\n",
    "                             pdstrn_df.elements.Month.alias(\"Month\"),\n",
    "                             pdstrn_df.elements.Mdate.alias(\"Mdate\"),\n",
    "                             pdstrn_df.elements.Day.alias(\"Day\"),\n",
    "                             pdstrn_df.elements.Time.alias(\"Time\"),\n",
    "                             pdstrn_df.elements.Sensor_ID.alias(\"Sensor_ID\"),\n",
    "                             pdstrn_df.elements.Sensor_Name.alias(\"Sensor_Name\"),\n",
    "                             pdstrn_df.elements.Hourly_Counts.alias(\"Hourly_Counts\"))\n",
    "\n",
    "pdstrn_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Date_Time: timestamp (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Mdate: integer (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Time: integer (nullable = true)\n",
      " |-- Sensor_ID: integer (nullable = true)\n",
      " |-- Sensor_Name: string (nullable = true)\n",
      " |-- Hourly_Counts: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Changing the datatype of the attrbutes into the required datatypes as per the metadata\n",
    "\n",
    "# ID: Integer Type\n",
    "pdstrn_df = pdstrn_df.withColumn('ID', pdstrn_df['ID'].cast(IntegerType()))\n",
    "\n",
    "# Date_Time: Timestamp\n",
    "pdstrn_df = pdstrn_df.withColumn(\"Date_Time\", F.to_timestamp(pdstrn_df.Date_Time, 'MM/dd/yyyy hh:mm:ss a'))\n",
    "\n",
    "# Year: Integer Type\n",
    "pdstrn_df = pdstrn_df.withColumn('Year', pdstrn_df['Year'].cast(IntegerType()))\n",
    "\n",
    "# Mdate: Integer Type\n",
    "pdstrn_df = pdstrn_df.withColumn('Mdate', pdstrn_df['Mdate'].cast(IntegerType()))\n",
    "\n",
    "# Time: Integer Type\n",
    "pdstrn_df = pdstrn_df.withColumn('Time', pdstrn_df['Time'].cast(IntegerType()))\n",
    "\n",
    "# Sensor_ID: Integer Type\n",
    "pdstrn_df = pdstrn_df.withColumn('Sensor_ID', pdstrn_df['Sensor_ID'].cast(IntegerType()))\n",
    "\n",
    "# Hourly_Counts: Integer Type\n",
    "pdstrn_df = pdstrn_df.withColumn('Hourly_Counts', pdstrn_df['Hourly_Counts'].cast(IntegerType()))\n",
    "\n",
    "pdstrn_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Transformations to prepare the columns for model prediction <a class=\"anchor\" name=\"6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdstrn_df = pdstrn_df.withColumn('next_date', F.date_add(F.to_date(F.col(\"Date_Time\")), 1))\n",
    "\n",
    "# Adding 1 day to the Date_Time column and also converting it to 'date' datatype\n",
    "pdstrn_df = pdstrn_df.withColumn('next_date', F.date_add('Date_Time', 1))\n",
    "\n",
    "# Extracting the month from the next_date attribute\n",
    "pdstrn_df = pdstrn_df.withColumn('next_Mdate', F.dayofmonth('next_date'))\n",
    "\n",
    "# Extracting the week from the next_date attribute\n",
    "pdstrn_df = pdstrn_df.withColumn('next_day_week', F.weekofyear('next_date'))\n",
    "\n",
    "# Extracting the day of week from the next_date attribute\n",
    "# and also making sure that Monday is the first day of the week\n",
    "pdstrn_df = pdstrn_df.withColumn('next_day_of_week', F.when(F.dayofweek('next_date') == 0, 7)\\\n",
    "                                                     .otherwise(F.dayofweek('next_date') - 1))\n",
    "\n",
    "# Changing the name of the Hourly_Counts column to 'prev_count'\n",
    "pdstrn_df = pdstrn_df.withColumnRenamed('Hourly_Counts', 'prev_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if the schema of the dataframe is what is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Date_Time: timestamp (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Mdate: integer (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Time: integer (nullable = true)\n",
      " |-- Sensor_ID: integer (nullable = true)\n",
      " |-- Sensor_Name: string (nullable = true)\n",
      " |-- prev_count: integer (nullable = true)\n",
      " |-- next_date: date (nullable = true)\n",
      " |-- next_Mdate: integer (nullable = true)\n",
      " |-- next_day_week: integer (nullable = true)\n",
      " |-- next_day_of_week: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdstrn_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering the data on the basis of Time attribute. The hours to be included in the data need to be between 9 and 23 hours. This can be done using the **filter()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the data based on the Time attribute\n",
    "pdstrn_df = pdstrn_df.filter((pdstrn_df.Time >= 9) & \\\n",
    "                             (pdstrn_df.Time <= 23))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Modelling Building <a class=\"anchor\" name=\"7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we have to be able to use the given model and use that to predict pedestrian counts for the data provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifying the file path for process model\n",
    "model_path = 'count_estimation_pipeline_model'\n",
    "\n",
    "#Loading to given model into the pipeline\n",
    "pipeline_model = PipelineModel.load(model_path) \n",
    "\n",
    "#Transforming using the pipeline model\n",
    "prediction_df = pipeline_model.transform(pdstrn_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to write the prediction data to the sink in paraquet format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the things used:\n",
    "* Removal of the checkpoint: Though theoretically there is no need to remove the checkpoint files, but it is causing problems when writing the data to the sink. It was giving errors on the sparkUI. Therefore the code to remove the checkpoint file has been added.\n",
    "    * rm: To remove the file\n",
    "    * -rf : -r to remove the file and -f to force it in a quiet manner ie. without display.\n",
    "* sparks writestream API\n",
    "* To write the data in parquet format we need to use the the format() function with **'parquet'** parameter.\n",
    "* outputMode - 'append' : In this mode only the new rows will added to the result table.\n",
    "* option(path): The file path where the result table would get stored.\n",
    "* option(checkpointLocation): With checkpointing the state it would help to recover the last state.\n",
    "* start() : To start writing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf parquet_prediction_df/checkpoint\n",
    "\n",
    "pdstrn_prediction_sink = prediction_df.writeStream.format(\"parquet\")\\\n",
    "                                      .outputMode(\"append\")\\\n",
    "                                      .option(\"path\", \"parquet_prediction_df\")\\\n",
    "                                      .option(\"checkpointLocation\", \"parquet_prediction_df/checkpoint\")\\\n",
    "                                      .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the file sink query after some time\n",
    "# pdstrn_prediction_sink.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Aggregating data <a class=\"anchor\" name=\"8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a) Aggregating on days <a class=\"anchor\" name=\"8a\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we would need to:\n",
    "* Filter the data to get only the rows where the prediction is greater than 2000.\n",
    "* Convert the next_date attribute to timestamp datatype so that the watermarking can be applied to it.\n",
    "* Watermarking for 1 day to get complete records of that day.\n",
    "* Group by or aggregate  based on the window for each day. This can be done using the timestamp column. The aggregation woul dbe done on the basis of the sensor_id column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the data for predictions more than 2000\n",
    "analyse_df = prediction_df.filter(prediction_df.prediction > 2000)\n",
    "\n",
    "# Converting the next_date attribute to timestamp datatype so that the watermarking can be applied to it\n",
    "analyse_df= analyse_df.withColumn('next_date', F.col('next_date').cast('timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating the data based on the counts of hours which have\n",
    "# predictions > 2000\n",
    "# Watermarking for 1 day has been used\n",
    "# Window for each day has been considered using the window function\n",
    "# The aggregation is done usingthe agg() function\n",
    "df_hours_count = analyse_df.withWatermark('next_date', '1 day')\\\n",
    "                    .groupBy(F.window('next_date','1 day'),'Sensor_ID')\\\n",
    "                    .agg(F.count('Sensor_ID').alias('CountofHours'))\\\n",
    "                    .select(\"Sensor_ID\", \"window\",\"CountofHours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is in line we will try to check the schema of the data before we persist it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Sensor_ID: integer (nullable = true)\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- CountofHours: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hours_count.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the data in the notebook, one of the ways is to use the **foreachBatch()** functio and the oher way is to use the **format()** as `memory` and in the this case we have used the latter.\n",
    "\n",
    "**Please Note: One might have to wait for the entire data to be dislayed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hours_aggregated = df_hours_count \\\n",
    "                        .writeStream \\\n",
    "                        .outputMode(\"complete\") \\\n",
    "                        .format('memory')\\\n",
    "                        .queryName('display_in_noebook')\\\n",
    "                        .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the things used:\n",
    "* sparks writestream API\n",
    "* outputMode - 'complete' : In this mode the entire updated Result Table will be written to the external storage.\n",
    "* format(): The format in which the data needs to be, the `memory` format has been chosen in this case.\n",
    "* queryName(): Using this as the table name the data can be queried.\n",
    "* start() : To start writing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below query needs to be run multiple times, as it takes time for the data to displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------------+------------+\n",
      "|Sensor_ID|window                                    |CountofHours|\n",
      "+---------+------------------------------------------+------------+\n",
      "|1        |[2020-12-02 00:00:00, 2020-12-03 00:00:00]|4           |\n",
      "|2        |[2020-12-02 00:00:00, 2020-12-03 00:00:00]|6           |\n",
      "|4        |[2020-12-02 00:00:00, 2020-12-03 00:00:00]|4           |\n",
      "|5        |[2020-12-02 00:00:00, 2020-12-03 00:00:00]|2           |\n",
      "|6        |[2020-12-02 00:00:00, 2020-12-03 00:00:00]|3           |\n",
      "|41       |[2020-12-02 00:00:00, 2020-12-03 00:00:00]|1           |\n",
      "|58       |[2020-12-02 00:00:00, 2020-12-03 00:00:00]|2           |\n",
      "|59       |[2020-12-02 00:00:00, 2020-12-03 00:00:00]|3           |\n",
      "|63       |[2020-12-02 00:00:00, 2020-12-03 00:00:00]|4           |\n",
      "|65       |[2020-12-02 00:00:00, 2020-12-03 00:00:00]|2           |\n",
      "|67       |[2020-12-02 00:00:00, 2020-12-03 00:00:00]|3           |\n",
      "|69       |[2020-12-02 00:00:00, 2020-12-03 00:00:00]|1           |\n",
      "|1        |[2020-12-03 00:00:00, 2020-12-04 00:00:00]|5           |\n",
      "|2        |[2020-12-03 00:00:00, 2020-12-04 00:00:00]|6           |\n",
      "|4        |[2020-12-03 00:00:00, 2020-12-04 00:00:00]|6           |\n",
      "|22       |[2020-12-03 00:00:00, 2020-12-04 00:00:00]|3           |\n",
      "|24       |[2020-12-03 00:00:00, 2020-12-04 00:00:00]|1           |\n",
      "|41       |[2020-12-03 00:00:00, 2020-12-04 00:00:00]|5           |\n",
      "|58       |[2020-12-03 00:00:00, 2020-12-04 00:00:00]|1           |\n",
      "|59       |[2020-12-03 00:00:00, 2020-12-04 00:00:00]|1           |\n",
      "|63       |[2020-12-03 00:00:00, 2020-12-04 00:00:00]|3           |\n",
      "|65       |[2020-12-03 00:00:00, 2020-12-04 00:00:00]|2           |\n",
      "|67       |[2020-12-03 00:00:00, 2020-12-04 00:00:00]|1           |\n",
      "|68       |[2020-12-03 00:00:00, 2020-12-04 00:00:00]|2           |\n",
      "|1        |[2020-12-04 00:00:00, 2020-12-05 00:00:00]|8           |\n",
      "|2        |[2020-12-04 00:00:00, 2020-12-05 00:00:00]|6           |\n",
      "|4        |[2020-12-04 00:00:00, 2020-12-05 00:00:00]|6           |\n",
      "|22       |[2020-12-04 00:00:00, 2020-12-05 00:00:00]|2           |\n",
      "|35       |[2020-12-04 00:00:00, 2020-12-05 00:00:00]|4           |\n",
      "|41       |[2020-12-04 00:00:00, 2020-12-05 00:00:00]|10          |\n",
      "|59       |[2020-12-04 00:00:00, 2020-12-05 00:00:00]|1           |\n",
      "|63       |[2020-12-04 00:00:00, 2020-12-05 00:00:00]|1           |\n",
      "|65       |[2020-12-04 00:00:00, 2020-12-05 00:00:00]|1           |\n",
      "|67       |[2020-12-04 00:00:00, 2020-12-05 00:00:00]|1           |\n",
      "|68       |[2020-12-04 00:00:00, 2020-12-05 00:00:00]|2           |\n",
      "|69       |[2020-12-04 00:00:00, 2020-12-05 00:00:00]|1           |\n",
      "|1        |[2020-12-05 00:00:00, 2020-12-06 00:00:00]|7           |\n",
      "|2        |[2020-12-05 00:00:00, 2020-12-06 00:00:00]|5           |\n",
      "|4        |[2020-12-05 00:00:00, 2020-12-06 00:00:00]|7           |\n",
      "|5        |[2020-12-05 00:00:00, 2020-12-06 00:00:00]|2           |\n",
      "|35       |[2020-12-05 00:00:00, 2020-12-06 00:00:00]|7           |\n",
      "|41       |[2020-12-05 00:00:00, 2020-12-06 00:00:00]|11          |\n",
      "|1        |[2020-12-06 00:00:00, 2020-12-07 00:00:00]|8           |\n",
      "|2        |[2020-12-06 00:00:00, 2020-12-07 00:00:00]|6           |\n",
      "|4        |[2020-12-06 00:00:00, 2020-12-07 00:00:00]|9           |\n",
      "|5        |[2020-12-06 00:00:00, 2020-12-07 00:00:00]|3           |\n",
      "|6        |[2020-12-06 00:00:00, 2020-12-07 00:00:00]|1           |\n",
      "|22       |[2020-12-06 00:00:00, 2020-12-07 00:00:00]|7           |\n",
      "|24       |[2020-12-06 00:00:00, 2020-12-07 00:00:00]|1           |\n",
      "|35       |[2020-12-06 00:00:00, 2020-12-07 00:00:00]|6           |\n",
      "|41       |[2020-12-06 00:00:00, 2020-12-07 00:00:00]|11          |\n",
      "|47       |[2020-12-06 00:00:00, 2020-12-07 00:00:00]|2           |\n",
      "|53       |[2020-12-06 00:00:00, 2020-12-07 00:00:00]|2           |\n",
      "|55       |[2020-12-06 00:00:00, 2020-12-07 00:00:00]|3           |\n",
      "|57       |[2020-12-06 00:00:00, 2020-12-07 00:00:00]|3           |\n",
      "|58       |[2020-12-06 00:00:00, 2020-12-07 00:00:00]|2           |\n",
      "|59       |[2020-12-06 00:00:00, 2020-12-07 00:00:00]|2           |\n",
      "|61       |[2020-12-06 00:00:00, 2020-12-07 00:00:00]|4           |\n",
      "|62       |[2020-12-06 00:00:00, 2020-12-07 00:00:00]|3           |\n",
      "|63       |[2020-12-06 00:00:00, 2020-12-07 00:00:00]|2           |\n",
      "|65       |[2020-12-06 00:00:00, 2020-12-07 00:00:00]|2           |\n",
      "|66       |[2020-12-06 00:00:00, 2020-12-07 00:00:00]|3           |\n",
      "|67       |[2020-12-06 00:00:00, 2020-12-07 00:00:00]|2           |\n",
      "|68       |[2020-12-06 00:00:00, 2020-12-07 00:00:00]|4           |\n",
      "|69       |[2020-12-06 00:00:00, 2020-12-07 00:00:00]|5           |\n",
      "|73       |[2020-12-06 00:00:00, 2020-12-07 00:00:00]|2           |\n",
      "|1        |[2020-12-07 00:00:00, 2020-12-08 00:00:00]|6           |\n",
      "|2        |[2020-12-07 00:00:00, 2020-12-08 00:00:00]|5           |\n",
      "|4        |[2020-12-07 00:00:00, 2020-12-08 00:00:00]|6           |\n",
      "|5        |[2020-12-07 00:00:00, 2020-12-08 00:00:00]|2           |\n",
      "|6        |[2020-12-07 00:00:00, 2020-12-08 00:00:00]|2           |\n",
      "|22       |[2020-12-07 00:00:00, 2020-12-08 00:00:00]|7           |\n",
      "|24       |[2020-12-07 00:00:00, 2020-12-08 00:00:00]|1           |\n",
      "|35       |[2020-12-07 00:00:00, 2020-12-08 00:00:00]|2           |\n",
      "|41       |[2020-12-07 00:00:00, 2020-12-08 00:00:00]|6           |\n",
      "|55       |[2020-12-07 00:00:00, 2020-12-08 00:00:00]|1           |\n",
      "|57       |[2020-12-07 00:00:00, 2020-12-08 00:00:00]|3           |\n",
      "|58       |[2020-12-07 00:00:00, 2020-12-08 00:00:00]|4           |\n",
      "|59       |[2020-12-07 00:00:00, 2020-12-08 00:00:00]|2           |\n",
      "|61       |[2020-12-07 00:00:00, 2020-12-08 00:00:00]|5           |\n",
      "|62       |[2020-12-07 00:00:00, 2020-12-08 00:00:00]|3           |\n",
      "|63       |[2020-12-07 00:00:00, 2020-12-08 00:00:00]|2           |\n",
      "|65       |[2020-12-07 00:00:00, 2020-12-08 00:00:00]|2           |\n",
      "|66       |[2020-12-07 00:00:00, 2020-12-08 00:00:00]|3           |\n",
      "|67       |[2020-12-07 00:00:00, 2020-12-08 00:00:00]|4           |\n",
      "|68       |[2020-12-07 00:00:00, 2020-12-08 00:00:00]|6           |\n",
      "|69       |[2020-12-07 00:00:00, 2020-12-08 00:00:00]|4           |\n",
      "|1        |[2020-12-08 00:00:00, 2020-12-09 00:00:00]|2           |\n",
      "|2        |[2020-12-08 00:00:00, 2020-12-09 00:00:00]|6           |\n",
      "|4        |[2020-12-08 00:00:00, 2020-12-09 00:00:00]|4           |\n",
      "|6        |[2020-12-08 00:00:00, 2020-12-09 00:00:00]|4           |\n",
      "|24       |[2020-12-08 00:00:00, 2020-12-09 00:00:00]|1           |\n",
      "|41       |[2020-12-08 00:00:00, 2020-12-09 00:00:00]|4           |\n",
      "|58       |[2020-12-08 00:00:00, 2020-12-09 00:00:00]|4           |\n",
      "|59       |[2020-12-08 00:00:00, 2020-12-09 00:00:00]|3           |\n",
      "|63       |[2020-12-08 00:00:00, 2020-12-09 00:00:00]|1           |\n",
      "|65       |[2020-12-08 00:00:00, 2020-12-09 00:00:00]|3           |\n",
      "|67       |[2020-12-08 00:00:00, 2020-12-09 00:00:00]|2           |\n",
      "|68       |[2020-12-08 00:00:00, 2020-12-09 00:00:00]|1           |\n",
      "|69       |[2020-12-08 00:00:00, 2020-12-09 00:00:00]|1           |\n",
      "|1        |[2020-12-09 00:00:00, 2020-12-10 00:00:00]|3           |\n",
      "|2        |[2020-12-09 00:00:00, 2020-12-10 00:00:00]|5           |\n",
      "|4        |[2020-12-09 00:00:00, 2020-12-10 00:00:00]|6           |\n",
      "|22       |[2020-12-09 00:00:00, 2020-12-10 00:00:00]|1           |\n",
      "|24       |[2020-12-09 00:00:00, 2020-12-10 00:00:00]|1           |\n",
      "|41       |[2020-12-09 00:00:00, 2020-12-10 00:00:00]|7           |\n",
      "|59       |[2020-12-09 00:00:00, 2020-12-10 00:00:00]|2           |\n",
      "|61       |[2020-12-09 00:00:00, 2020-12-10 00:00:00]|1           |\n",
      "|65       |[2020-12-09 00:00:00, 2020-12-10 00:00:00]|1           |\n",
      "|69       |[2020-12-09 00:00:00, 2020-12-10 00:00:00]|1           |\n",
      "+---------+------------------------------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * from display_in_noebook ORDER BY window, sensor_id').show(500, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping the query after the required number\n",
    "# of records get displayed in the output\n",
    "# df_hours_aggregated.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the former way that is through the **foreachBatch()** function . The below function needs to be called in side the **foreachBatch()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code for the function that can be used in\n",
    "# the foreachBatch() function to display the results.\n",
    "# def display_in_notebook(batch, epoch_id):\n",
    "    \n",
    "#     '''\n",
    "#     This function takes care of the display of batch records.\n",
    "#     Params:\n",
    "#     1. batch - The batch data.\n",
    "#     '''\n",
    "    \n",
    "#     batch.show(500, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b) Publishing the data to kafka topic <a class=\"anchor\" name=\"8b\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we would need to:\n",
    "* Join the dataframe created in the last part with the dataframe with sensor locations data which can be done using **join()** function.\n",
    "* Next we need to Select th columns - window.start, Sensor_ID, latitude, longitude.\n",
    "* Also make the window.start as key and concatenate others as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_joined_df = df_hours_count.join(sensor_spdf,df_hours_count.Sensor_ID == sensor_spdf.sensor_id, how='inner')\\\n",
    "                                .select(df_hours_count.window.start.cast('string').alias('key')\\\n",
    "                                        ,F.concat(sensor_spdf.latitude.cast('string')\\\n",
    "                                        ,F.lit(','),sensor_spdf.longitude.cast('string')\\\n",
    "                                        ,F.lit(','),sensor_spdf.sensor_id).alias('value'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to write the data to a kafka topic which can be done by using the below things:\n",
    "* Removal of the checkpoint: Though theoretically there is no need to remove the checkpoint files, but it is causing problems when publishing the data to kafka topic and consuming the data for the next Task, i.e. Task 3. Therefore the code to remove the checkpoint file has been added.\n",
    "    * rm: To remove the file\n",
    "    * -rf : -r to remove the file and -f to force it in a quiet manner ie. without display.\n",
    "* sparks writestream API\n",
    "* To write the data in kafka format we need to use the the format() function with **'kafka'** parameter.\n",
    "* outputMode - 'append' : In this mode only the new rows will added to the result table.\n",
    "* option(\"kafka.bootstrap.servers\"): To set the server that needs to be used.\n",
    "* option(checkpointLocation): With checkpointing the state it would help to recover the last state.\n",
    "* option(topic): To se the kafka topic that the data needs to be written to.\n",
    "* start() : To start writing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf final_joined_df/checkpoint\n",
    "\n",
    "final_joined_stream = final_joined_df.writeStream \\\n",
    "                                     .format(\"kafka\") \\\n",
    "                                     .outputMode(\"append\")\\\n",
    "                                     .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "                                     .option(\"checkpointLocation\", \"final_joined_df/checkpoint\")\\\n",
    "                                     .option(\"topic\", \"final_joined_df\") \\\n",
    "                                     .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the stream after the visualization has been done for Task3\n",
    "# final_joined_stream.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
